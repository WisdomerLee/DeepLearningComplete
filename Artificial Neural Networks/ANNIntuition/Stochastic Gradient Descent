gradient descent라는 방법으로 cost function이 최소가 되는 지점을 찾을 수 있는데 
문제는 이 접근방식으로는 단일 차원의 함수일 경우에 가능하다는 것임...
실제로는 훨씬 더 복잡한 함수들로 다차원 공간에서 cost function이 정의되는 경우가 많은데 이 경우에 gradient descent를 적용하려면...?
그러면 중간중간 local minimum들이 여럿 있는 복잡한 함수들이 있게 되는데 이러면 중간에 local minimum에 갇힐 수 있게 됨...

이를 해결하기 위해 stochastic gradient descent라는 기법이 등장

gradient descent는 하나의 neural network로 각각의 입력 값에 대한 출력값들과 실제 값의 차이를 이용하여 cost function의 gradient를 계산하여 back propagation으로 보정
즉 단일 neural network를 이용하여 다른 값을 계산
stochastic gradient descent기법은 조금 다른데
하나의 neural network로 입력 값과 출력값, 실제 값의 차이로 cost function의 gradient를 계산, back propagation으로 weight를 재조정
즉, 하나의 입력, 출력을 얻을 때마다 neural network가 조정되어 다음에 또 쓰이게 된다 : 같은 neural network가 아닌 계속 조정된 neural network로 보정시킴
값을 얻을 때마다 즉각 보정이 일어나는 것이 stochastic gradient descent!!
