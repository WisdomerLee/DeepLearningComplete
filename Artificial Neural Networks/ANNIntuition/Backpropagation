Backpropagation??

input layer > hidden layers > output layer 이 방향으로 진행되는 것을 forwardpropagation
거꾸로 진행되는 것을 backpropagation이라 부름

ANN with Stochastic Gradient Descent
1. 0에 가깝게 작은 숫자를 random하게 뽑아 wieght를 줌
2. input layer에 집어넣은 dataset의 값을 보고 나온 값을 확인, 하나의 입력 노드에서 나온 각각의 속성들이 전달됨
3. forward-propagation : neuron은 해당 방향으로 weight에 의존하여 활성화되어 서로에게 영향을 줌 > 결과 값이 나올 때까지 neuron의 계산 진행
4. 실제 결과와 예측으로 나온 값 비교 : error가 측정
5. back-propagation : error가 back-propagated되어 error에 얼마나 기여했는지를 확인하여 weight를 다시 계산함
learning rate는 weight를 각각 얼마나 조정할지를 결정!
6. 1~5를 반복하고 각각의 출력마다 weight를 업데이트하기 : Reinforcement learning, 
 1~5를 반복하고 obervation이 특정 batch만큼 진행되었을 때 weight를 업데이트하기 : Batch learning
7. ANN으로 훈련된 값이 나왔을 때 epoch를 만듦, 다시 epoch를 실행!

이 모든 과정은 cost function을 최소화 하는 방향으로 진행
